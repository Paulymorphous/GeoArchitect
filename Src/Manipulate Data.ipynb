{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = img_height = 256\n",
    "num_channels = 3\n",
    "root_path = \"../Data/Roads/\"\n",
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files = 27324\n"
     ]
    }
   ],
   "source": [
    "files = next(os.walk(\"../Data/Roads/train\"+'/sat2/'))[2]\n",
    "print('Total number of files =',len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_masks = []\n",
    "all_images = []\n",
    "rej_count = 0\n",
    "\n",
    "def crop_and_save():\n",
    "    \"\"\"\n",
    "    Imports images and crops to multiple sub images of a definite size inorder to preserve the resolution of the images and maximise the number of images that are available.\n",
    "    \"\"\"\n",
    "    \n",
    "    for image_file in tqdm_notebook(files, total = len(files)):\n",
    "       \n",
    "   \n",
    "        image_path = root_path+mode+'/sat2/'+image_file\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        mask_path = root_path+mode+'/map2/'+image_file\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        if (len(np.unique(mask)) == 1):\n",
    "            rej_count += 1\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        all_images.append(image)\n",
    "        all_masks.append(mask)  \n",
    "              \n",
    "        all_images = np.asarray(all_images)\n",
    "        all_masks = np.asarray(all_masks)\n",
    "        \n",
    "        print('{} images were rejected.'.format(rej_count))\n",
    "        print(\"Shape of Train Images =\", all_images.shape)\n",
    "        print(\"Shape of Train Labels =\", all_masks.shape)\n",
    "        print(\"Memory size of Image array = \", all_images.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b512d6ee234f4b9a66953f982c2406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27324), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Train Images = (27324, 256, 256, 3)\n",
      "Shape of Train Labels = (27324, 256, 256)\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot serialize a bytes object larger than 4 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-58a9e2c82dbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages_to_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-58a9e2c82dbf>\u001b[0m in \u001b[0;36mimages_to_pickle\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/Roads/Pickled_Images/new_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_images.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/Roads/Pickled_Images/new_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_masks.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: cannot serialize a bytes object larger than 4 GiB"
     ]
    }
   ],
   "source": [
    "def images_to_pickle():\n",
    "    all_masks = []\n",
    "    all_images = []\n",
    " \n",
    "\n",
    "    for image_file in tqdm_notebook(files, total = len(files)):\n",
    "        \n",
    "   \n",
    "        image_path = root_path+mode+'/sat2/'+image_file\n",
    "        \n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        #image = cv2.resize(image, (img_height, img_width), interpolation = cv2.INTER_AREA)\n",
    "        #image = np.stack((image,)*1, axis=-1)\n",
    "        all_images.append(image)\n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    \n",
    "        mask_path = root_path+mode+'/map2/'+image_file\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        #mask = cv2.resize(mask, (img_height, img_width), interpolation = cv2.INTER_AREA)\n",
    "        #mask  = np.stack((mask,)*1, axis=-1)\n",
    "    \n",
    "        all_masks.append(mask)  \n",
    "    \n",
    "    all_images = np.asarray(all_images)\n",
    "    all_masks = np.asarray(all_masks)\n",
    "    \n",
    "    print(\"Shape of Train Images =\", all_images.shape)\n",
    "    print(\"Shape of Train Labels =\", all_masks.shape)\n",
    "    \n",
    "    with open('../Data/Roads/Pickled_Images/new_'+mode+'_images.pickle', 'wb') as outfile:\n",
    "        pickle.dump(all_images, outfile)\n",
    "    \n",
    "    with open('../Data/Roads/Pickled_Images/new_'+mode+'_masks.pickle', 'wb') as outfile:\n",
    "        pickle.dump(all_masks, outfile)\n",
    "    \n",
    "    print(\"Data has been successfully exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks = images_to_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('../Data/Roads/Pickled_Images/new_'+mode+'_images.h5py', 'w') as hf:\n",
    "    hf.create_dataset(\"all_images\",  data=all_images)\n",
    "    \n",
    "with h5py.File('../Data/Roads/Pickled_Images/new_'+mode+'_masks.h5py', 'w') as hf:\n",
    "    hf.create_dataset(\"all_masks\",  data=all_masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
